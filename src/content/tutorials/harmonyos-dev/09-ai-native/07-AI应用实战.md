---
title: AI åº”ç”¨å®æˆ˜
date: 2025-01-22
prevChapter: "harmonyos-dev/09-ai-native/06-AIæ¨¡å‹é›†æˆ"
parentChapter: "harmonyos-dev/09-ai-native/README"
---
# AI åº”ç”¨å®æˆ˜

> ç»¼åˆAIèƒ½åŠ›çš„å®æˆ˜é¡¹ç›®

## ğŸ¤ æ™ºèƒ½è¯­éŸ³åŠ©æ‰‹

### å®Œæ•´å®ç°

```typescript
@Entry
@Component
struct SmartAssistant {
  @State transcript: string = ''
  @State response: string = ''
  @State isListening: boolean = false
  @State conversation: Message[] = []
  
  private recognizer: SpeechRecognizer
  private tts: TTS
  private nlp: NLPEngine
  
  async aboutToAppear() {
    await this.initAI()
  }
  
  async initAI() {
    // åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«
    this.recognizer = speechRecognizer.create({
      language: 'zh-CN',
      continuous: false
    })
    
    // åˆå§‹åŒ–è¯­éŸ³åˆæˆ
    this.tts = textToSpeech.create({
      language: 'zh-CN',
      voice: 'female'
    })
    
    // åˆå§‹åŒ–NLP
    this.nlp = nlpEngine.create()
  }
  
  async startListening() {
    this.isListening = true
    
    this.recognizer.on('result', async (result) => {
      const userInput = result.text
      this.transcript = userInput
      
      // æ·»åŠ åˆ°å¯¹è¯å†å²
      this.conversation.push({
        role: 'user',
        content: userInput,
        timestamp: Date.now()
      })
      
      // å¤„ç†æŒ‡ä»¤
      const response = await this.processCommand(userInput)
      
      // æ·»åŠ åŠ©æ‰‹å›å¤
      this.conversation.push({
        role: 'assistant',
        content: response,
        timestamp: Date.now()
      })
      
      // è¯­éŸ³æ’­æŠ¥
      await this.speak(response)
      
      this.isListening = false
    })
    
    this.recognizer.start()
  }
  
  async processCommand(command: string): Promise<string> {
    // 1. æ„å›¾è¯†åˆ«
    const intent = await this.nlp.detectIntent(command)
    
    switch (intent.type) {
      case 'WEATHER':
        return await this.getWeather(intent.location)
      
      case 'TIME':
        return this.getTime()
      
      case 'REMINDER':
        return await this.setReminder(intent.params)
      
      case 'SEARCH':
        return await this.search(intent.query)
      
      case 'TRANSLATE':
        return await this.translate(intent.text, intent.targetLang)
      
      default:
        return 'æŠ±æ­‰ï¼Œæˆ‘è¿˜ä¸èƒ½ç†è§£è¿™ä¸ªæŒ‡ä»¤'
    }
  }
  
  async getWeather(location: string): Promise<string> {
    // è°ƒç”¨å¤©æ°”API
    const weather = await weatherService.get(location)
    return `${location}ä»Šå¤©${weather.condition}ï¼Œæ¸©åº¦${weather.temperature}åº¦`
  }
  
  getTime(): string {
    const now = new Date()
    return `ç°åœ¨æ˜¯${now.getHours()}ç‚¹${now.getMinutes()}åˆ†`
  }
  
  async setReminder(params: any): Promise<string> {
    await reminderService.create(params)
    return `å¥½çš„ï¼Œæˆ‘å·²ç»è®¾ç½®äº†æé†’`
  }
  
  async search(query: string): Promise<string> {
    const results = await searchService.search(query)
    return `æˆ‘ä¸ºä½ æ‰¾åˆ°äº†${results.length}æ¡ç»“æœ`
  }
  
  async translate(text: string, targetLang: string): Promise<string> {
    const result = await translator.translate(text, targetLang)
    return `ç¿»è¯‘ç»“æœï¼š${result.text}`
  }
  
  async speak(text: string) {
    await this.tts.speak(text)
  }
  
  build() {
    Column({ space: 20 }) {
      // æ ‡é¢˜
      Text('æ™ºèƒ½è¯­éŸ³åŠ©æ‰‹')
        .fontSize(28)
        .fontWeight(FontWeight.Bold)
      
      // å¯¹è¯å†å²
      List({ space: 15 }) {
        ForEach(this.conversation, (msg: Message) => {
          ListItem() {
            Row() {
              if (msg.role === 'assistant') {
                Blank()
              }
              
              Column({ space: 5 }) {
                Text(msg.content)
                  .fontSize(16)
                  .padding(15)
                  .backgroundColor(msg.role === 'user' ? 0x3B82F6 : 0xE5E7EB)
                  .fontColor(msg.role === 'user' ? Color.White : Color.Black)
                  .borderRadius(12)
                  .maxWidth('80%')
                
                Text(new Date(msg.timestamp).toLocaleTimeString())
                  .fontSize(12)
                  .fontColor(Color.Gray)
              }
              
              if (msg.role === 'user') {
                Blank()
              }
            }
            .width('100%')
          }
        })
      }
      .layoutWeight(1)
      .scrollBar(BarState.Off)
      
      // æ§åˆ¶æŒ‰é’®
      Button(this.isListening ? 'æ­£åœ¨å¬...' : 'ç‚¹å‡»è¯´è¯')
        .width(200)
        .height(60)
        .fontSize(18)
        .backgroundColor(this.isListening ? Color.Red : Color.Blue)
        .onClick(() => {
          if (!this.isListening) {
            this.startListening()
          }
        })
    }
    .width('100%')
    .height('100%')
    .padding(20)
    .backgroundColor(0xF5F5F5)
  }
}

interface Message {
  role: 'user' | 'assistant'
  content: string
  timestamp: number
}
```

## ğŸ“¸ æ™ºèƒ½ç›¸å†Œ

### AI ç…§ç‰‡ç®¡ç†

```typescript
@Entry
@Component
struct SmartAlbum {
  @State photos: SmartPhoto[] = []
  @State searchQuery: string = ''
  @State selectedTags: string[] = []
  
  private classifier: ImageClassifier
  private faceDetector: FaceDetector
  
  async aboutToAppear() {
    await this.initAI()
    await this.scanPhotos()
  }
  
  async initAI() {
    this.classifier = imageClassifier.create()
    this.faceDetector = faceDetector.create()
  }
  
  async scanPhotos() {
    const allPhotos = await photoLibrary.getAll()
    
    for (const photo of allPhotos) {
      const imageData = await photo.getData()
      
      // å›¾åƒåˆ†ç±»
      const tags = await this.classifier.classify(imageData)
      
      // äººè„¸æ£€æµ‹
      const faces = await this.faceDetector.detect(imageData)
      
      // OCR æ–‡å­—è¯†åˆ«
      const text = await ocrEngine.recognize(imageData)
      
      this.photos.push({
        id: photo.id,
        path: photo.path,
        tags: tags.map(t => t.label),
        faces: faces.length,
        text: text,
        timestamp: photo.timestamp
      })
    }
  }
  
  async searchPhotos(query: string) {
    return this.photos.filter(photo => {
      // æ ‡ç­¾åŒ¹é…
      const tagMatch = photo.tags.some(tag => 
        tag.toLowerCase().includes(query.toLowerCase())
      )
      
      // æ–‡å­—åŒ¹é…
      const textMatch = photo.text && 
        photo.text.toLowerCase().includes(query.toLowerCase())
      
      return tagMatch || textMatch
    })
  }
  
  build() {
    Column() {
      // æœç´¢æ 
      SearchBar({
        placeholder: 'æœç´¢ç…§ç‰‡ï¼ˆå¦‚ï¼šçŒ«ã€æµ·æ»©ã€æ–‡å­—ï¼‰',
        onSearch: async (query) => {
          const results = await this.searchPhotos(query)
          this.photos = results
        }
      })
      
      // æ ‡ç­¾è¿‡æ»¤
      Scroll(Axis.Horizontal) {
        Row({ space: 10 }) {
          ForEach(['äººç‰©', 'é£æ™¯', 'ç¾é£Ÿ', 'å® ç‰©', 'å»ºç­‘'], (tag: string) => {
            Button(tag)
              .fontSize(14)
              .onClick(() => {
                this.searchPhotos(tag)
              })
          })
        }
      }
      .padding({ left: 15, right: 15 })
      
      // ç…§ç‰‡ç½‘æ ¼
      Grid() {
        ForEach(this.photos, (photo: SmartPhoto) => {
          GridItem() {
            Column() {
              Image(photo.path)
                .width('100%')
                .aspectRatio(1)
                .borderRadius(8)
              
              // æ ‡ç­¾
              Text(photo.tags.slice(0, 2).join(', '))
                .fontSize(12)
                .maxLines(1)
            }
          }
        })
      }
      .columnsTemplate('1fr 1fr 1fr')
      .rowsGap(10)
      .columnsGap(10)
      .padding(15)
    }
  }
}
```

## ğŸŒ å®æ—¶ç¿»è¯‘å™¨

### å¤šæ¨¡æ€ç¿»è¯‘

```typescript
@Entry
@Component
struct MultiModalTranslator {
  @State sourceText: string = ''
  @State translatedText: string = ''
  @State mode: 'text' | 'voice' | 'camera' = 'text'
  
  private translator: Translator
  private recognizer: SpeechRecognizer
  private ocrEngine: OCREngine
  
  async translateText(text: string) {
    this.sourceText = text
    const result = await this.translator.translate(text, {
      from: 'auto',
      to: 'en-US'
    })
    this.translatedText = result.text
  }
  
  async translateVoice() {
    this.recognizer.on('result', async (result) => {
      await this.translateText(result.text)
    })
    this.recognizer.start()
  }
  
  async translateCamera(imagePath: string) {
    const imageData = await loadImage(imagePath)
    const text = await this.ocrEngine.recognize(imageData)
    await this.translateText(text)
  }
  
  build() {
    Column() {
      // æ¨¡å¼åˆ‡æ¢
      Tabs() {
        TabContent() {
          TextTranslator({ onTranslate: this.translateText.bind(this) })
        }.tabBar('æ–‡å­—')
        
        TabContent() {
          VoiceTranslator({ onTranslate: this.translateVoice.bind(this) })
        }.tabBar('è¯­éŸ³')
        
        TabContent() {
          CameraTranslator({ onTranslate: this.translateCamera.bind(this) })
        }.tabBar('æ‹ç…§')
      }
    }
  }
}
```

## ğŸ’¡ æ€§èƒ½ä¼˜åŒ–

### 1. æ¨¡å‹é¢„åŠ è½½

```typescript
// åº”ç”¨å¯åŠ¨æ—¶é¢„åŠ è½½
async function preloadModels() {
  await Promise.all([
    imageClassifier.create(),
    speechRecognizer.create(),
    translator.create()
  ])
}
```

### 2. ç»“æœç¼“å­˜

```typescript
const cache = new LRUCache<string, any>(100)

async function classifyWithCache(image: ImageData) {
  const key = getImageHash(image)
  
  if (cache.has(key)) {
    return cache.get(key)
  }
  
  const result = await classifier.classify(image)
  cache.set(key, result)
  return result
}
```

### 3. åå°å¤„ç†

```typescript
import worker from '@ohos.worker'

// åœ¨ Worker ä¸­å¤„ç†è€—æ—¶ä»»åŠ¡
const aiWorker = new worker.ThreadWorker('workers/ai_worker.ts')

aiWorker.postMessage({ type: 'classify', image: imageData })

aiWorker.on('message', (result) => {
  console.log('åˆ†ç±»ç»“æœ:', result)
})
```

## ğŸ“š å‚è€ƒèµ„æº

- [AIèƒ½åŠ›å¼€å‘æŒ‡å—](https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V5/ai-development-intro-0000001820880589-V5)

---

**ç¬¬9ç« å®Œæˆï¼** ç»§ç»­å­¦ä¹  â†’ [ç¬¬10ç« ï¼šåˆ†å¸ƒå¼èƒ½åŠ›å‡çº§](../10-distributed-next/)
