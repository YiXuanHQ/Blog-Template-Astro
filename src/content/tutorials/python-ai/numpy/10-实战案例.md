---
title: å®æˆ˜æ¡ˆä¾‹
prevChapter: "python-ai/numpy/09-æ•°ç»„é«˜çº§æ“ä½œ"
parentChapter: "python-ai/numpy/README"
---
# å®æˆ˜æ¡ˆä¾‹

> é€šè¿‡å®æˆ˜é¡¹ç›®å·©å›ºNumPyçŸ¥è¯†

## ğŸ“š å­¦ä¹ ç›®æ ‡

- åº”ç”¨NumPyè§£å†³å®é™…é—®é¢˜
- æŒæ¡æ•°æ®å¤„ç†æŠ€å·§
- å­¦ä¼šæ€§èƒ½ä¼˜åŒ–
- ç»¼åˆè¿ç”¨æ‰€å­¦çŸ¥è¯†

## 1. å›¾åƒå¤„ç†

### æ¡ˆä¾‹1ï¼šå›¾åƒç°åº¦åŒ–

```python
import numpy as np

# æ¨¡æ‹Ÿå½©è‰²å›¾åƒï¼ˆRGBï¼‰
height, width = 100, 100
image_rgb = np.random.randint(0, 256, (height, width, 3), dtype=np.uint8)

# ç°åº¦åŒ–å…¬å¼ï¼šGray = 0.299*R + 0.587*G + 0.114*B
weights = np.array([0.299, 0.587, 0.114])
image_gray = np.dot(image_rgb, weights).astype(np.uint8)

print(f'å½©è‰²å›¾åƒå½¢çŠ¶: {image_rgb.shape}')
print(f'ç°åº¦å›¾åƒå½¢çŠ¶: {image_gray.shape}')
```

### æ¡ˆä¾‹2ï¼šå›¾åƒæ»¤æ³¢

```python
# å‡å€¼æ»¤æ³¢ï¼ˆ3x3ï¼‰
def mean_filter(image):
    h, w = image.shape
    filtered = np.zeros_like(image)
    
    for i in range(1, h-1):
        for j in range(1, w-1):
            filtered[i, j] = np.mean(image[i-1:i+2, j-1:j+2])
    
    return filtered

# ä½¿ç”¨å·ç§¯å®ç°ï¼ˆæ›´å¿«ï¼‰
def mean_filter_fast(image):
    from scipy.ndimage import convolve
    kernel = np.ones((3, 3)) / 9
    return convolve(image, kernel)
```

## 2. æ•°æ®åˆ†æ

### æ¡ˆä¾‹1ï¼šå­¦ç”Ÿæˆç»©åˆ†æ

```python
# ç”Ÿæˆå­¦ç”Ÿæˆç»©æ•°æ®
np.random.seed(42)
students = 100
subjects = 5

scores = np.random.normal(75, 10, (students, subjects))
scores = np.clip(scores, 0, 100)  # é™åˆ¶åœ¨0-100

# ç»Ÿè®¡åˆ†æ
print('=== æˆç»©ç»Ÿè®¡ ===')
print(f'å¹³å‡åˆ†: {scores.mean():.2f}')
print(f'æœ€é«˜åˆ†: {scores.max():.2f}')
print(f'æœ€ä½åˆ†: {scores.min():.2f}')
print(f'æ ‡å‡†å·®: {scores.std():.2f}')

# å„ç§‘å¹³å‡åˆ†
subject_means = scores.mean(axis=0)
for i, mean in enumerate(subject_means, 1):
    print(f'ç§‘ç›®{i}å¹³å‡åˆ†: {mean:.2f}')

# æ¯ä¸ªå­¦ç”Ÿçš„æ€»åˆ†å’Œå¹³å‡åˆ†
total_scores = scores.sum(axis=1)
student_means = scores.mean(axis=1)

# æ’å
rankings = np.argsort(total_scores)[::-1]
print(f'\nå‰5åå­¦ç”Ÿ: {rankings[:5]}')
print(f'å‰5åæ€»åˆ†: {total_scores[rankings[:5]]}')

# åˆ†æ•°æ®µç»Ÿè®¡
bins = [0, 60, 70, 80, 90, 100]
labels = ['ä¸åŠæ ¼', 'åŠæ ¼', 'ä¸­ç­‰', 'è‰¯å¥½', 'ä¼˜ç§€']

for i in range(subjects):
    hist, _ = np.histogram(scores[:, i], bins=bins)
    print(f'\nç§‘ç›®{i+1}åˆ†æ•°æ®µåˆ†å¸ƒ:')
    for label, count in zip(labels, hist):
        print(f'  {label}: {count}äºº ({count/students*100:.1f}%)')
```

### æ¡ˆä¾‹2ï¼šæ—¶é—´åºåˆ—åˆ†æ

```python
# ç”Ÿæˆè‚¡ç¥¨ä»·æ ¼æ•°æ®
np.random.seed(42)
days = 252  # ä¸€å¹´äº¤æ˜“æ—¥
start_price = 100

# æ¨¡æ‹Ÿéšæœºæ¸¸èµ°
returns = np.random.normal(0.001, 0.02, days)
prices = start_price * np.exp(np.cumsum(returns))

# è®¡ç®—ç§»åŠ¨å¹³å‡
def moving_average(data, window):
    weights = np.ones(window) / window
    return np.convolve(data, weights, mode='valid')

ma5 = moving_average(prices, 5)
ma20 = moving_average(prices, 20)

# è®¡ç®—æ³¢åŠ¨ç‡
def volatility(prices, window=20):
    returns = np.diff(np.log(prices))
    vol = np.zeros(len(prices))
    for i in range(window, len(prices)):
        vol[i] = np.std(returns[i-window:i]) * np.sqrt(252)
    return vol

vol = volatility(prices)

print(f'æœŸåˆä»·æ ¼: {prices[0]:.2f}')
print(f'æœŸæœ«ä»·æ ¼: {prices[-1]:.2f}')
print(f'æ”¶ç›Šç‡: {(prices[-1]/prices[0] - 1)*100:.2f}%')
print(f'æœ€å¤§å›æ’¤: {((prices.max() - prices.min())/prices.max())*100:.2f}%')
```

## 3. æœºå™¨å­¦ä¹ é¢„å¤„ç†

### æ¡ˆä¾‹1ï¼šæ•°æ®æ ‡å‡†åŒ–

```python
# ç”Ÿæˆæ•°æ®
data = np.random.randn(1000, 10) * 100 + 50

# Z-scoreæ ‡å‡†åŒ–
def z_score_normalize(data):
    mean = data.mean(axis=0)
    std = data.std(axis=0)
    return (data - mean) / std

# Min-Maxå½’ä¸€åŒ–
def min_max_normalize(data):
    min_val = data.min(axis=0)
    max_val = data.max(axis=0)
    return (data - min_val) / (max_val - min_val)

# L2å½’ä¸€åŒ–
def l2_normalize(data):
    norms = np.linalg.norm(data, axis=1, keepdims=True)
    return data / norms

data_zscore = z_score_normalize(data)
data_minmax = min_max_normalize(data)
data_l2 = l2_normalize(data)

print('åŸå§‹æ•°æ®èŒƒå›´:', data.min(), '~', data.max())
print('Z-scoreåèŒƒå›´:', data_zscore.min(), '~', data_zscore.max())
print('Min-MaxåèŒƒå›´:', data_minmax.min(), '~', data_minmax.max())
```

### æ¡ˆä¾‹2ï¼šç‰¹å¾å·¥ç¨‹

```python
# å¤šé¡¹å¼ç‰¹å¾
def polynomial_features(X, degree=2):
    """ç”Ÿæˆå¤šé¡¹å¼ç‰¹å¾"""
    n_samples, n_features = X.shape
    features = [X]
    
    for d in range(2, degree + 1):
        features.append(X ** d)
    
    return np.hstack(features)

X = np.array([[1, 2], [3, 4], [5, 6]])
X_poly = polynomial_features(X, degree=2)
print('åŸå§‹ç‰¹å¾:\n', X)
print('å¤šé¡¹å¼ç‰¹å¾:\n', X_poly)

# ç‰¹å¾é€‰æ‹©ï¼ˆæ–¹å·®è¿‡æ»¤ï¼‰
def variance_threshold(X, threshold=0.0):
    """ç§»é™¤ä½æ–¹å·®ç‰¹å¾"""
    variances = X.var(axis=0)
    return X[:, variances > threshold]
```

## 4. æ•°å€¼è®¡ç®—

### æ¡ˆä¾‹1ï¼šçŸ©é˜µè¿ç®—ä¼˜åŒ–

```python
import time

n = 1000

# æ–¹å¼1ï¼šä½¿ç”¨å¾ªç¯ï¼ˆæ…¢ï¼‰
A = np.random.rand(n, n)
B = np.random.rand(n, n)

start = time.time()
C = np.zeros((n, n))
for i in range(n):
    for j in range(n):
        for k in range(n):
            C[i, j] += A[i, k] * B[k, j]
loop_time = time.time() - start

# æ–¹å¼2ï¼šä½¿ç”¨NumPyï¼ˆå¿«ï¼‰
start = time.time()
C = A @ B
numpy_time = time.time() - start

print(f'å¾ªç¯è€—æ—¶: {loop_time:.4f}ç§’')
print(f'NumPyè€—æ—¶: {numpy_time:.4f}ç§’')
print(f'åŠ é€Ÿæ¯”: {loop_time/numpy_time:.1f}x')
```

### æ¡ˆä¾‹2ï¼šæ±‚è§£æœ€å°äºŒä¹˜é—®é¢˜

```python
# çº¿æ€§å›å½’ï¼šy = ax + b
np.random.seed(42)
x = np.linspace(0, 10, 100)
y = 2 * x + 1 + np.random.randn(100) * 2

# æ„é€ è®¾è®¡çŸ©é˜µ
X = np.vstack([x, np.ones(len(x))]).T

# æ±‚è§£æœ€å°äºŒä¹˜
params = np.linalg.lstsq(X, y, rcond=None)[0]
a, b = params

print(f'æ‹Ÿåˆç»“æœ: y = {a:.2f}x + {b:.2f}')

# è®¡ç®—RÂ²
y_pred = a * x + b
ss_res = np.sum((y - y_pred) ** 2)
ss_tot = np.sum((y - y.mean()) ** 2)
r2 = 1 - ss_res / ss_tot
print(f'RÂ²: {r2:.4f}')
```

## 5. ç®—æ³•å®ç°

### æ¡ˆä¾‹1ï¼šK-meansèšç±»

```python
def kmeans(X, k, max_iters=100):
    """ç®€å•çš„K-meanså®ç°"""
    n_samples = X.shape[0]
    
    # éšæœºåˆå§‹åŒ–ä¸­å¿ƒç‚¹
    indices = np.random.choice(n_samples, k, replace=False)
    centers = X[indices]
    
    for _ in range(max_iters):
        # åˆ†é…æ ·æœ¬åˆ°æœ€è¿‘çš„ä¸­å¿ƒ
        distances = np.sqrt(((X - centers[:, np.newaxis])**2).sum(axis=2))
        labels = np.argmin(distances, axis=0)
        
        # æ›´æ–°ä¸­å¿ƒç‚¹
        new_centers = np.array([X[labels == i].mean(axis=0) for i in range(k)])
        
        # æ£€æŸ¥æ”¶æ•›
        if np.allclose(centers, new_centers):
            break
        
        centers = new_centers
    
    return labels, centers

# æµ‹è¯•
np.random.seed(42)
X = np.vstack([
    np.random.randn(100, 2) + [2, 2],
    np.random.randn(100, 2) + [-2, -2],
    np.random.randn(100, 2) + [2, -2]
])

labels, centers = kmeans(X, k=3)
print('èšç±»ä¸­å¿ƒ:\n', centers)
```

### æ¡ˆä¾‹2ï¼šæ¢¯åº¦ä¸‹é™

```python
def gradient_descent(X, y, learning_rate=0.01, epochs=1000):
    """çº¿æ€§å›å½’çš„æ¢¯åº¦ä¸‹é™"""
    n_samples, n_features = X.shape
    weights = np.zeros(n_features)
    bias = 0
    
    for epoch in range(epochs):
        # å‰å‘ä¼ æ’­
        y_pred = X @ weights + bias
        
        # è®¡ç®—æ¢¯åº¦
        dw = (1/n_samples) * X.T @ (y_pred - y)
        db = (1/n_samples) * np.sum(y_pred - y)
        
        # æ›´æ–°å‚æ•°
        weights -= learning_rate * dw
        bias -= learning_rate * db
        
        # æ¯100è½®æ‰“å°æŸå¤±
        if epoch % 100 == 0:
            loss = np.mean((y_pred - y) ** 2)
            print(f'Epoch {epoch}, Loss: {loss:.4f}')
    
    return weights, bias

# æµ‹è¯•
np.random.seed(42)
X = np.random.randn(100, 3)
true_weights = np.array([1, 2, 3])
y = X @ true_weights + np.random.randn(100) * 0.1

weights, bias = gradient_descent(X, y)
print('çœŸå®æƒé‡:', true_weights)
print('å­¦ä¹ æƒé‡:', weights)
```

## 6. æ€§èƒ½ä¼˜åŒ–æ€»ç»“

```python
# 1. ä½¿ç”¨å‘é‡åŒ–æ“ä½œ
arr = np.random.rand(1000)

# âŒ æ…¢
result = np.array([x**2 for x in arr])

# âœ… å¿«
result = arr ** 2

# 2. é¿å…ä¸å¿…è¦çš„å¤åˆ¶
arr = np.random.rand(1000, 1000)

# âŒ åˆ›å»ºå‰¯æœ¬
arr_copy = arr.copy()

# âœ… ä½¿ç”¨è§†å›¾
arr_view = arr[:]

# 3. ä½¿ç”¨åˆé€‚çš„æ•°æ®ç±»å‹
# âŒ ä½¿ç”¨float64ï¼ˆ8å­—èŠ‚ï¼‰
arr_float64 = np.random.rand(1000000)

# âœ… ä½¿ç”¨float32ï¼ˆ4å­—èŠ‚ï¼‰
arr_float32 = np.random.rand(1000000).astype(np.float32)

print(f'float64å†…å­˜: {arr_float64.nbytes / 1024 / 1024:.2f}MB')
print(f'float32å†…å­˜: {arr_float32.nbytes / 1024 / 1024:.2f}MB')

# 4. é¢„åˆ†é…æ•°ç»„
# âŒ åŠ¨æ€å¢é•¿
result = np.array([])
for i in range(1000):
    result = np.append(result, i)

# âœ… é¢„åˆ†é…
result = np.zeros(1000)
for i in range(1000):
    result[i] = i
```

## ğŸ’¡ æ€»ç»“

é€šè¿‡è¿™äº›å®æˆ˜æ¡ˆä¾‹ï¼Œæˆ‘ä»¬å­¦ä¹ äº†ï¼š

1. **å›¾åƒå¤„ç†** - ç°åº¦åŒ–ã€æ»¤æ³¢
2. **æ•°æ®åˆ†æ** - ç»Ÿè®¡åˆ†æã€æ—¶é—´åºåˆ—
3. **æœºå™¨å­¦ä¹ ** - æ•°æ®é¢„å¤„ç†ã€ç‰¹å¾å·¥ç¨‹
4. **æ•°å€¼è®¡ç®—** - çŸ©é˜µè¿ç®—ã€ä¼˜åŒ–ç®—æ³•
5. **ç®—æ³•å®ç°** - K-meansã€æ¢¯åº¦ä¸‹é™
6. **æ€§èƒ½ä¼˜åŒ–** - å‘é‡åŒ–ã€å†…å­˜ç®¡ç†

## ç»ƒä¹ é¡¹ç›®

1. å®ç°å›¾åƒçš„è¾¹ç¼˜æ£€æµ‹ï¼ˆSobelç®—å­ï¼‰
2. ç¼–å†™ä¸€ä¸ªç®€å•çš„æ¨èç³»ç»Ÿï¼ˆååŒè¿‡æ»¤ï¼‰
3. å®ç°PCAé™ç»´ç®—æ³•
4. å¼€å‘ä¸€ä¸ªæ•°æ®æ¸…æ´—å·¥å…·

---

**æ­å–œå®Œæˆ NumPy æ•™ç¨‹ï¼** ğŸ‰

ç»§ç»­å­¦ä¹ ï¼š[Pandas æ•°æ®åˆ†æ](../pandas/)
