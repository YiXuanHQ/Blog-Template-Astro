---
title: æ–‡æœ¬ç”Ÿæˆ
prevChapter: "python-ai/nlp/10-Transformer"
nextChapter: "python-ai/nlp/12-å®æˆ˜æ¡ˆä¾‹"
parentChapter: "python-ai/nlp/README"
---
# æ–‡æœ¬ç”Ÿæˆ

> æŒæ¡æ–‡æœ¬ç”ŸæˆæŠ€æœ¯å’Œåº”ç”¨

## ğŸ“š å­¦ä¹ ç›®æ ‡

- ç†è§£è¯­è¨€æ¨¡å‹
- æŒæ¡æ–‡æœ¬ç”Ÿæˆæ–¹æ³•
- å­¦ä¼šæ–‡æœ¬æ‘˜è¦
- äº†è§£å¯¹è¯ç³»ç»Ÿ

## 1. è¯­è¨€æ¨¡å‹

```python
import torch
import torch.nn as nn

class LanguageModel(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, vocab_size)
    
    def forward(self, x, hidden=None):
        embedded = self.embedding(x)
        output, hidden = self.lstm(embedded, hidden)
        logits = self.fc(output)
        return logits, hidden

# ç”Ÿæˆæ–‡æœ¬
def generate_text(model, start_text, length=50):
    model.eval()
    with torch.no_grad():
        # åˆå§‹åŒ–
        input_ids = tokenize(start_text)
        generated = input_ids.copy()
        
        for _ in range(length):
            x = torch.tensor([generated]).long()
            output, _ = model(x)
            next_token = output[0, -1].argmax().item()
            generated.append(next_token)
        
        return decode(generated)
```

## 2. ä½¿ç”¨ GPT ç”Ÿæˆ

```python
from transformers import GPT2LMHeadModel, GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')

def generate(prompt, max_length=100):
    input_ids = tokenizer.encode(prompt, return_tensors='pt')
    
    output = model.generate(
        input_ids,
        max_length=max_length,
        num_beams=5,              # beam search
        no_repeat_ngram_size=2,   # é¿å…é‡å¤
        early_stopping=True,
        temperature=0.8           # æ§åˆ¶éšæœºæ€§
    )
    
    text = tokenizer.decode(output[0], skip_special_tokens=True)
    return text

result = generate("The future of AI is")
print(result)
```

## 3. æ–‡æœ¬æ‘˜è¦

```python
from transformers import pipeline

# ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

text = """
Long article text here...
"""

summary = summarizer(text, max_length=130, min_length=30, do_sample=False)
print(summary[0]['summary_text'])
```

## 4. åºåˆ—åˆ°åºåˆ—

```python
class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
    
    def forward(self, src, trg):
        # ç¼–ç 
        encoder_outputs, hidden = self.encoder(src)
        
        # è§£ç 
        outputs, hidden = self.decoder(trg, hidden)
        
        return outputs

# ç”¨äºæœºå™¨ç¿»è¯‘
# src: "I love you"
# trg: "æˆ‘çˆ±ä½ "
```

## 5. å¯¹è¯ç”Ÿæˆ

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# åŠ è½½å¯¹è¯æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained("microsoft/DialoGPT-medium")
tokenizer = AutoTokenizer.from_pretrained("microsoft/DialoGPT-medium")

# å¤šè½®å¯¹è¯
chat_history_ids = None

while True:
    user_input = input("You: ")
    if user_input.lower() == 'quit':
        break
    
    # ç¼–ç ç”¨æˆ·è¾“å…¥
    new_input_ids = tokenizer.encode(user_input + tokenizer.eos_token,
                                     return_tensors='pt')
    
    # æ‹¼æ¥å†å²
    if chat_history_ids is not None:
        bot_input_ids = torch.cat([chat_history_ids, new_input_ids], dim=-1)
    else:
        bot_input_ids = new_input_ids
    
    # ç”Ÿæˆå›å¤
    chat_history_ids = model.generate(
        bot_input_ids,
        max_length=1000,
        pad_token_id=tokenizer.eos_token_id
    )
    
    # è§£ç 
    response = tokenizer.decode(
        chat_history_ids[:, bot_input_ids.shape[-1]:][0],
        skip_special_tokens=True
    )
    print(f"Bot: {response}")
```

---

**ä¸‹ä¸€èŠ‚ï¼š** [å®æˆ˜æ¡ˆä¾‹](12-å®æˆ˜æ¡ˆä¾‹.md)
