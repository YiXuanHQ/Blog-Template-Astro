---
title: æ–‡æœ¬åˆ†ç±»
prevChapter: "python-ai/nlp/04-è¯å‘é‡"
nextChapter: "python-ai/nlp/06-æƒ…æ„Ÿåˆ†æ"
parentChapter: "python-ai/nlp/README"
---
# æ–‡æœ¬åˆ†ç±»

> æŒæ¡æ–‡æœ¬åˆ†ç±»ç®—æ³•å’Œåº”ç”¨

## ğŸ“š å­¦ä¹ ç›®æ ‡

- æŒæ¡æœ´ç´ è´å¶æ–¯åˆ†ç±»
- å­¦ä¼šSVMæ–‡æœ¬åˆ†ç±»
- äº†è§£æ·±åº¦å­¦ä¹ åˆ†ç±»
- æŒæ¡è¯„ä¼°æŒ‡æ ‡

## 1. æœ´ç´ è´å¶æ–¯

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
import jieba

# å‡†å¤‡æ•°æ®
texts = [
    "è¿™éƒ¨ç”µå½±å¤ªå¥½çœ‹äº†",
    "è¿™å®¶é¤å…å¾ˆä¸é”™",
    "è¿™ä¸ªæ‰‹æœºå¾ˆåƒåœ¾",
    "æœåŠ¡æ€åº¦å¤ªå·®äº†"
]
labels = [1, 1, 0, 0]  # 1-æ­£é¢, 0-è´Ÿé¢

# åˆ†è¯
texts = [' '.join(jieba.cut(text)) for text in texts]

# ç‰¹å¾æå–
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(texts)

# åˆ’åˆ†æ•°æ®é›†
X_train, X_test, y_train, y_test = train_test_split(
    X, labels, test_size=0.2, random_state=42
)

# è®­ç»ƒæ¨¡å‹
clf = MultinomialNB()
clf.fit(X_train, y_train)

# é¢„æµ‹
predictions = clf.predict(X_test)
print("å‡†ç¡®ç‡:", clf.score(X_test, y_test))

# é¢„æµ‹æ–°æ–‡æœ¬
new_text = "è¿™ä¸ªäº§å“è´¨é‡å¾ˆå¥½"
new_text_seg = ' '.join(jieba.cut(new_text))
new_X = vectorizer.transform([new_text_seg])
pred = clf.predict(new_X)
print(f"'{new_text}' é¢„æµ‹ä¸º:", "æ­£é¢" if pred[0] == 1 else "è´Ÿé¢")
```

## 2. SVM åˆ†ç±»

```python
from sklearn.svm import SVC

# è®­ç»ƒSVM
clf = SVC(kernel='linear', C=1.0)
clf.fit(X_train, y_train)

# é¢„æµ‹
predictions = clf.predict(X_test)
print("SVMå‡†ç¡®ç‡:", clf.score(X_test, y_test))
```

## 3. æ·±åº¦å­¦ä¹ åˆ†ç±»

```python
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset

# ç®€å•çš„æ–‡æœ¬åˆ†ç±»å™¨
class TextClassifier(nn.Module):
    def __init__(self, vocab_size, embed_dim, num_classes):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.lstm = nn.LSTM(embed_dim, 128, batch_first=True)
        self.fc = nn.Linear(128, num_classes)
    
    def forward(self, x):
        x = self.embedding(x)
        _, (h, _) = self.lstm(x)
        out = self.fc(h[-1])
        return out

# è®­ç»ƒè¿‡ç¨‹
model = TextClassifier(vocab_size=10000, embed_dim=100, num_classes=2)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# è®­ç»ƒå¾ªç¯
for epoch in range(10):
    for batch_x, batch_y in train_loader:
        optimizer.zero_grad()
        outputs = model(batch_x)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
```

## 4. è¯„ä¼°æŒ‡æ ‡

```python
from sklearn.metrics import classification_report, confusion_matrix

# è¯¦ç»†æŠ¥å‘Š
print(classification_report(y_test, predictions))

# æ··æ·†çŸ©é˜µ
cm = confusion_matrix(y_test, predictions)
print("æ··æ·†çŸ©é˜µ:\n", cm)

# å‡†ç¡®ç‡ã€å¬å›ç‡ã€F1
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

print("å‡†ç¡®ç‡:", accuracy_score(y_test, predictions))
print("ç²¾ç¡®ç‡:", precision_score(y_test, predictions))
print("å¬å›ç‡:", recall_score(y_test, predictions))
print("F1å€¼:", f1_score(y_test, predictions))
```

---

**ä¸‹ä¸€èŠ‚ï¼š** [æƒ…æ„Ÿåˆ†æ](06-æƒ…æ„Ÿåˆ†æ.md)
