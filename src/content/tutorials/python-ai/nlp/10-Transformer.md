---
title: Transformer
prevChapter: "python-ai/nlp/09-RNNå’ŒLSTM"
nextChapter: "python-ai/nlp/11-æ–‡æœ¬ç”Ÿæˆ"
parentChapter: "python-ai/nlp/README"
---
# Transformer

> æŒæ¡Transformeræ¶æ„å’Œé¢„è®­ç»ƒæ¨¡å‹

## ğŸ“š å­¦ä¹ ç›®æ ‡

- ç†è§£Attentionæœºåˆ¶
- æŒæ¡Transformeræ¶æ„
- å­¦ä¼šä½¿ç”¨BERT
- äº†è§£GPTæ¨¡å‹

## 1. Self-Attention

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class SelfAttention(nn.Module):
    def __init__(self, embed_dim):
        super().__init__()
        self.embed_dim = embed_dim
        self.query = nn.Linear(embed_dim, embed_dim)
        self.key = nn.Linear(embed_dim, embed_dim)
        self.value = nn.Linear(embed_dim, embed_dim)
    
    def forward(self, x):
        # x: (batch, seq_len, embed_dim)
        Q = self.query(x)
        K = self.key(x)
        V = self.value(x)
        
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.embed_dim ** 0.5)
        attention_weights = F.softmax(scores, dim=-1)
        
        # åŠ æƒæ±‚å’Œ
        out = torch.matmul(attention_weights, V)
        return out
```

## 2. ä½¿ç”¨ BERT

```python
from transformers import BertTokenizer, BertModel

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
model = BertModel.from_pretrained('bert-base-chinese')

# ç¼–ç æ–‡æœ¬
text = "è‡ªç„¶è¯­è¨€å¤„ç†å¾ˆæœ‰è¶£"
inputs = tokenizer(text, return_tensors='pt')

# è·å–ç¼–ç 
outputs = model(**inputs)
last_hidden_states = outputs.last_hidden_state
print(last_hidden_states.shape)  # (1, seq_len, 768)
```

## 3. BERT æ–‡æœ¬åˆ†ç±»

```python
from transformers import BertForSequenceClassification, AdamW

# åŠ è½½åˆ†ç±»æ¨¡å‹
model = BertForSequenceClassification.from_pretrained(
    'bert-base-chinese',
    num_labels=2
)

# è®­ç»ƒ
optimizer = AdamW(model.parameters(), lr=2e-5)

for epoch in range(3):
    for batch in train_loader:
        inputs = tokenizer(batch['text'], padding=True, 
                          truncation=True, return_tensors='pt')
        labels = batch['labels']
        
        outputs = model(**inputs, labels=labels)
        loss = outputs.loss
        
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
```

## 4. ä½¿ç”¨ GPT

```python
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# åŠ è½½GPT-2
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')

# ç”Ÿæˆæ–‡æœ¬
input_text = "Once upon a time"
input_ids = tokenizer.encode(input_text, return_tensors='pt')

# ç”Ÿæˆ
output = model.generate(
    input_ids,
    max_length=50,
    num_return_sequences=1,
    no_repeat_ngram_size=2,
    temperature=0.7
)

generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
print(generated_text)
```

## 5. ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹

```python
# ä½¿ç”¨Chinese-BERT
from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained("hfl/chinese-bert-wwm-ext")
model = AutoModel.from_pretrained("hfl/chinese-bert-wwm-ext")

# ç¼–ç 
text = "ä»Šå¤©å¤©æ°”å¾ˆå¥½"
inputs = tokenizer(text, return_tensors="pt")
outputs = model(**inputs)
```

---

**ä¸‹ä¸€èŠ‚ï¼š** [æ–‡æœ¬ç”Ÿæˆ](11-æ–‡æœ¬ç”Ÿæˆ.md)
