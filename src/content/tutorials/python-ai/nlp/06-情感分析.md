---
title: æƒ…æ„Ÿåˆ†æž
prevChapter: "python-ai/nlp/05-æ–‡æœ¬åˆ†ç±»"
nextChapter: "python-ai/nlp/07-å‘½åå®žä½“è¯†åˆ«"
parentChapter: "python-ai/nlp/README"
---
# æƒ…æ„Ÿåˆ†æž

> æŽŒæ¡æƒ…æ„Ÿåˆ†æžæŠ€æœ¯å’Œåº”ç”¨

## ðŸ“š å­¦ä¹ ç›®æ ‡

- ç†è§£æƒ…æ„Ÿåˆ†æž
- æŽŒæ¡åŸºäºŽè¯å…¸çš„æ–¹æ³•
- å­¦ä¼šæœºå™¨å­¦ä¹ æ–¹æ³•
- äº†è§£æ·±åº¦å­¦ä¹ æ–¹æ³•

## 1. åŸºäºŽè¯å…¸çš„æ–¹æ³•

```python
# ä½¿ç”¨SnowNLP
from snownlp import SnowNLP

text = "è¿™éƒ¨ç”µå½±çœŸçš„å¤ªå¥½çœ‹äº†ï¼"
s = SnowNLP(text)
print(f"æƒ…æ„Ÿå¾—åˆ†: {s.sentiments}")  # 0-1ä¹‹é—´

# è‡ªå®šä¹‰æƒ…æ„Ÿè¯å…¸
positive_words = {'å¥½', 'æ£’', 'ä¼˜ç§€', 'å–œæ¬¢'}
negative_words = {'å·®', 'çƒ‚', 'è®¨åŽŒ', 'å¤±æœ›'}

def sentiment_score(text):
    words = jieba.cut(text)
    pos_count = sum(1 for w in words if w in positive_words)
    neg_count = sum(1 for w in words if w in negative_words)
    
    if pos_count > neg_count:
        return 'positive'
    elif neg_count > pos_count:
        return 'negative'
    else:
        return 'neutral'
```

## 2. æœºå™¨å­¦ä¹ æ–¹æ³•

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

# è®­ç»ƒæ•°æ®
train_texts = [
    "è¿™ä¸ªäº§å“è´¨é‡å¾ˆå¥½",
    "æœåŠ¡æ€åº¦å¤ªå·®äº†",
    "æ€§ä»·æ¯”å¾ˆé«˜",
    "å®Œå…¨ä¸å€¼è¿™ä¸ªä»·æ ¼"
]
train_labels = [1, 0, 1, 0]  # 1-æ­£é¢, 0-è´Ÿé¢

# ç‰¹å¾æå–
vectorizer = TfidfVectorizer()
X_train = vectorizer.fit_transform(train_texts)

# è®­ç»ƒæ¨¡åž‹
clf = LogisticRegression()
clf.fit(X_train, train_labels)

# é¢„æµ‹
test_text = "è¿™ä¸ªæ‰‹æœºéžå¸¸å¥½ç”¨"
X_test = vectorizer.transform([test_text])
pred = clf.predict(X_test)
prob = clf.predict_proba(X_test)

print(f"é¢„æµ‹: {'æ­£é¢' if pred[0] == 1 else 'è´Ÿé¢'}")
print(f"æ¦‚çŽ‡: è´Ÿé¢={prob[0][0]:.2f}, æ­£é¢={prob[0][1]:.2f}")
```

## 3. ä½¿ç”¨transformers

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# åŠ è½½é¢„è®­ç»ƒæ¨¡åž‹
model_name = "uer/roberta-base-finetuned-chinaNews-chinese"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# é¢„æµ‹
text = "è¿™ä¸ªäº§å“çœŸçš„å¾ˆä¸é”™"
inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
outputs = model(**inputs)
predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)

print("æƒ…æ„Ÿé¢„æµ‹:", predictions)
```

## 4. å®žæˆ˜ï¼šç”µå•†è¯„è®ºåˆ†æž

```python
import pandas as pd
import matplotlib.pyplot as plt

# åŠ è½½è¯„è®ºæ•°æ®
reviews = pd.read_csv('reviews.csv')

# æ‰¹é‡æƒ…æ„Ÿåˆ†æž
reviews['sentiment'] = reviews['text'].apply(
    lambda x: SnowNLP(x).sentiments
)

# åˆ†ç±»
reviews['label'] = reviews['sentiment'].apply(
    lambda x: 'positive' if x > 0.6 else ('negative' if x < 0.4 else 'neutral')
)

# ç»Ÿè®¡
print(reviews['label'].value_counts())

# å¯è§†åŒ–
reviews['label'].value_counts().plot(kind='bar')
plt.title('Sentiment Distribution')
plt.show()
```

---

**ä¸‹ä¸€èŠ‚ï¼š** [å‘½åå®žä½“è¯†åˆ«](07-å‘½åå®žä½“è¯†åˆ«.md)
