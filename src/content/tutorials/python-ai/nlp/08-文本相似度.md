---
title: æ–‡æœ¬ç›¸ä¼¼åº¦
prevChapter: "python-ai/nlp/07-å‘½åå®ä½“è¯†åˆ«"
nextChapter: "python-ai/nlp/09-RNNå’ŒLSTM"
parentChapter: "python-ai/nlp/README"
---
# æ–‡æœ¬ç›¸ä¼¼åº¦

> æŒæ¡æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•

## ğŸ“š å­¦ä¹ ç›®æ ‡

- æŒæ¡ç¼–è¾‘è·ç¦»
- å­¦ä¼šä½™å¼¦ç›¸ä¼¼åº¦
- ç†è§£è¯­ä¹‰ç›¸ä¼¼åº¦
- äº†è§£åº”ç”¨åœºæ™¯

## 1. ç¼–è¾‘è·ç¦»

```python
def levenshtein_distance(s1, s2):
    m, n = len(s1), len(s2)
    dp = [[0] * (n + 1) for _ in range(m + 1)]
    
    for i in range(m + 1):
        dp[i][0] = i
    for j in range(n + 1):
        dp[0][j] = j
    
    for i in range(1, m + 1):
        for j in range(1, n + 1):
            if s1[i-1] == s2[j-1]:
                dp[i][j] = dp[i-1][j-1]
            else:
                dp[i][j] = min(
                    dp[i-1][j] + 1,    # åˆ é™¤
                    dp[i][j-1] + 1,    # æ’å…¥
                    dp[i-1][j-1] + 1   # æ›¿æ¢
                )
    
    return dp[m][n]

# ä½¿ç”¨
distance = levenshtein_distance("kitten", "sitting")
print(f"ç¼–è¾‘è·ç¦»: {distance}")
```

## 2. ä½™å¼¦ç›¸ä¼¼åº¦

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

texts = [
    "æˆ‘çˆ±è‡ªç„¶è¯­è¨€å¤„ç†",
    "æˆ‘å–œæ¬¢è‡ªç„¶è¯­è¨€å¤„ç†",
    "ä»Šå¤©å¤©æ°”å¾ˆå¥½"
]

# TF-IDFå‘é‡åŒ–
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(texts)

# è®¡ç®—ç›¸ä¼¼åº¦
similarity = cosine_similarity(tfidf_matrix)
print("ç›¸ä¼¼åº¦çŸ©é˜µ:\n", similarity)
```

## 3. Jaccard ç›¸ä¼¼åº¦

```python
def jaccard_similarity(s1, s2):
    set1 = set(s1)
    set2 = set(s2)
    intersection = set1 & set2
    union = set1 | set2
    return len(intersection) / len(union)

# ä½¿ç”¨
text1 = list(jieba.cut("æˆ‘çˆ±è‡ªç„¶è¯­è¨€å¤„ç†"))
text2 = list(jieba.cut("æˆ‘å–œæ¬¢è‡ªç„¶è¯­è¨€å¤„ç†"))
sim = jaccard_similarity(text1, text2)
print(f"Jaccardç›¸ä¼¼åº¦: {sim}")
```

## 4. è¯­ä¹‰ç›¸ä¼¼åº¦

```python
# ä½¿ç”¨Word2Vec
from gensim.models import Word2Vec

# è®­ç»ƒè¯å‘é‡
sentences = [["æˆ‘", "çˆ±", "ç¼–ç¨‹"], ["æˆ‘", "å–œæ¬¢", "ç¼–ç¨‹"]]
model = Word2Vec(sentences, vector_size=100, min_count=1)

# è®¡ç®—ç›¸ä¼¼åº¦
similarity = model.wv.similarity('çˆ±', 'å–œæ¬¢')
print(f"'çˆ±'å’Œ'å–œæ¬¢'çš„ç›¸ä¼¼åº¦: {similarity}")

# å¥å­ç›¸ä¼¼åº¦
import numpy as np

def sentence_similarity(s1, s2, model):
    words1 = [w for w in jieba.cut(s1) if w in model.wv]
    words2 = [w for w in jieba.cut(s2) if w in model.wv]
    
    if not words1 or not words2:
        return 0
    
    vec1 = np.mean([model.wv[w] for w in words1], axis=0)
    vec2 = np.mean([model.wv[w] for w in words2], axis=0)
    
    return cosine_similarity([vec1], [vec2])[0][0]
```

## 5. åº”ç”¨ï¼šæ–‡æœ¬æŸ¥é‡

```python
def find_similar_documents(query, documents, threshold=0.7):
    # å‘é‡åŒ–
    all_texts = [query] + documents
    vectorizer = TfidfVectorizer()
    tfidf = vectorizer.fit_transform(all_texts)
    
    # è®¡ç®—ç›¸ä¼¼åº¦
    similarities = cosine_similarity(tfidf[0:1], tfidf[1:]).flatten()
    
    # ç­›é€‰ç›¸ä¼¼æ–‡æ¡£
    similar_docs = []
    for idx, sim in enumerate(similarities):
        if sim >= threshold:
            similar_docs.append((idx, documents[idx], sim))
    
    return sorted(similar_docs, key=lambda x: x[2], reverse=True)

# ä½¿ç”¨
query = "è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯"
docs = [
    "NLPæŠ€æœ¯å‘å±•å¾ˆå¿«",
    "æ·±åº¦å­¦ä¹ åœ¨NLPä¸­åº”ç”¨å¹¿æ³›",
    "ä»Šå¤©å¤©æ°”å¾ˆå¥½"
]
results = find_similar_documents(query, docs, threshold=0.3)
for idx, doc, sim in results:
    print(f"æ–‡æ¡£{idx}: {doc} (ç›¸ä¼¼åº¦: {sim:.4f})")
```

---

**ä¸‹ä¸€èŠ‚ï¼š** [RNNå’ŒLSTM](09-RNNå’ŒLSTM.md)
