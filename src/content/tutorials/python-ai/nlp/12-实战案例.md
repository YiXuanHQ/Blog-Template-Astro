---
title: å®æˆ˜æ¡ˆä¾‹
prevChapter: "python-ai/nlp/11-æ–‡æœ¬ç”Ÿæˆ"
parentChapter: "python-ai/nlp/README"
---
# å®æˆ˜æ¡ˆä¾‹

> é€šè¿‡å®æˆ˜é¡¹ç›®æŒæ¡NLPæŠ€æœ¯

## ğŸ“š å­¦ä¹ ç›®æ ‡

- åº”ç”¨NLPè§£å†³å®é™…é—®é¢˜
- æŒæ¡å®Œæ•´é¡¹ç›®æµç¨‹
- å­¦ä¼šç»¼åˆè¿ç”¨æŠ€æœ¯

## 1. æ–°é—»åˆ†ç±»ç³»ç»Ÿ

```python
import jieba
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# 1. åŠ è½½æ•°æ®
df = pd.read_csv('news.csv')
print(f"æ•°æ®é‡: {len(df)}")
print(f"ç±»åˆ«: {df['category'].unique()}")

# 2. æ–‡æœ¬é¢„å¤„ç†
def preprocess(text):
    # åˆ†è¯
    words = jieba.cut(text)
    # å»é™¤åœç”¨è¯
    stopwords = set(open('stopwords.txt', encoding='utf-8').read().split())
    words = [w for w in words if w not in stopwords and len(w) > 1]
    return ' '.join(words)

df['text_processed'] = df['text'].apply(preprocess)

# 3. ç‰¹å¾æå–
vectorizer = TfidfVectorizer(max_features=5000)
X = vectorizer.fit_transform(df['text_processed'])
y = df['category']

# 4. åˆ’åˆ†æ•°æ®é›†
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 5. è®­ç»ƒæ¨¡å‹
clf = MultinomialNB()
clf.fit(X_train, y_train)

# 6. è¯„ä¼°
y_pred = clf.predict(X_test)
print("\nåˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test, y_pred))

# 7. é¢„æµ‹æ–°æ–‡æœ¬
def predict_category(text):
    text_processed = preprocess(text)
    text_vec = vectorizer.transform([text_processed])
    category = clf.predict(text_vec)[0]
    proba = clf.predict_proba(text_vec)[0]
    return category, max(proba)

# æµ‹è¯•
text = "ç§‘æŠ€å…¬å¸å‘å¸ƒæ–°äº§å“"
category, confidence = predict_category(text)
print(f"\né¢„æµ‹: {category} (ç½®ä¿¡åº¦: {confidence:.2%})")
```

## 2. è¯„è®ºæƒ…æ„Ÿåˆ†æ

```python
from snownlp import SnowNLP
import matplotlib.pyplot as plt

# åŠ è½½è¯„è®ºæ•°æ®
reviews = pd.read_csv('reviews.csv')

# æƒ…æ„Ÿåˆ†æ
def analyze_sentiment(text):
    s = SnowNLP(text)
    score = s.sentiments
    
    if score > 0.6:
        return 'positive', score
    elif score < 0.4:
        return 'negative', score
    else:
        return 'neutral', score

reviews[['sentiment', 'score']] = reviews['text'].apply(
    lambda x: pd.Series(analyze_sentiment(x))
)

# ç»Ÿè®¡åˆ†æ
print("\næƒ…æ„Ÿåˆ†å¸ƒ:")
print(reviews['sentiment'].value_counts())

print(f"\nå¹³å‡æƒ…æ„Ÿå¾—åˆ†: {reviews['score'].mean():.2f}")

# å¯è§†åŒ–
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
reviews['sentiment'].value_counts().plot(kind='bar')
plt.title('Sentiment Distribution')
plt.xlabel('Sentiment')
plt.ylabel('Count')

plt.subplot(1, 2, 2)
reviews['score'].hist(bins=30)
plt.title('Sentiment Score Distribution')
plt.xlabel('Score')
plt.ylabel('Frequency')

plt.tight_layout()
plt.savefig('sentiment_analysis.png')
plt.show()

# æå–å…³é”®è¯
from collections import Counter

positive_reviews = reviews[reviews['sentiment'] == 'positive']['text']
positive_words = []
for review in positive_reviews:
    positive_words.extend(jieba.cut(review))

word_freq = Counter(positive_words).most_common(20)
print("\næ­£é¢è¯„è®ºé«˜é¢‘è¯:")
for word, freq in word_freq:
    print(f"{word}: {freq}")
```

## 3. æ™ºèƒ½é—®ç­”ç³»ç»Ÿ

```python
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# çŸ¥è¯†åº“
qa_pairs = [
    {"question": "å¦‚ä½•é€€è´§ï¼Ÿ", "answer": "æ‚¨å¯ä»¥åœ¨è®¢å•è¯¦æƒ…é¡µé¢ç”³è¯·é€€è´§..."},
    {"question": "é…é€éœ€è¦å¤šä¹…ï¼Ÿ", "answer": "ä¸€èˆ¬3-5ä¸ªå·¥ä½œæ—¥é€è¾¾..."},
    {"question": "å¦‚ä½•è”ç³»å®¢æœï¼Ÿ", "answer": "å¯ä»¥æ‹¨æ‰“400ç”µè¯æˆ–åœ¨çº¿å’¨è¯¢..."}
]

# åŠ è½½æ¨¡å‹
model = SentenceTransformer('distiluse-base-multilingual-cased-v2')

# ç¼–ç é—®é¢˜
questions = [qa['question'] for qa in qa_pairs]
question_embeddings = model.encode(questions)

def find_answer(query, threshold=0.5):
    # ç¼–ç æŸ¥è¯¢
    query_embedding = model.encode([query])
    
    # è®¡ç®—ç›¸ä¼¼åº¦
    similarities = cosine_similarity(query_embedding, question_embeddings)[0]
    
    # æ‰¾åˆ°æœ€ç›¸ä¼¼çš„é—®é¢˜
    max_idx = np.argmax(similarities)
    max_sim = similarities[max_idx]
    
    if max_sim >= threshold:
        return qa_pairs[max_idx]['answer'], max_sim
    else:
        return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ç­”æ¡ˆ", 0.0

# æµ‹è¯•
while True:
    user_query = input("\nè¯·è¾“å…¥é—®é¢˜ (è¾“å…¥'quit'é€€å‡º): ")
    if user_query.lower() == 'quit':
        break
    
    answer, confidence = find_answer(user_query)
    print(f"\nå›ç­”: {answer}")
    print(f"ç½®ä¿¡åº¦: {confidence:.2f}")
```

## 4. æ–‡æœ¬æ‘˜è¦ç”Ÿæˆ

```python
from transformers import pipeline

# åŠ è½½æ‘˜è¦æ¨¡å‹
summarizer = pipeline("summarization", 
                     model="facebook/bart-large-cnn")

# é•¿æ–‡æœ¬
article = """
é•¿ç¯‡æ–‡ç« å†…å®¹...
è¿™é‡Œæ˜¯ä¸€ç¯‡å…³äºäººå·¥æ™ºèƒ½å‘å±•çš„æ–‡ç« ã€‚
äººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨è¿‘å¹´æ¥å–å¾—äº†çªç ´æ€§è¿›å±•...
"""

# ç”Ÿæˆæ‘˜è¦
def generate_summary(text, max_length=130, min_length=30):
    summary = summarizer(text, 
                        max_length=max_length,
                        min_length=min_length,
                        do_sample=False)
    return summary[0]['summary_text']

summary = generate_summary(article)
print("åŸæ–‡é•¿åº¦:", len(article))
print("\næ‘˜è¦:")
print(summary)
print("\næ‘˜è¦é•¿åº¦:", len(summary))
```

## 5. èŠå¤©æœºå™¨äºº

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

class ChatBot:
    def __init__(self, model_name="microsoft/DialoGPT-medium"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name)
        self.chat_history = None
    
    def respond(self, user_input):
        # ç¼–ç è¾“å…¥
        new_input = self.tokenizer.encode(
            user_input + self.tokenizer.eos_token,
            return_tensors='pt'
        )
        
        # æ‹¼æ¥å†å²
        if self.chat_history is not None:
            bot_input = torch.cat([self.chat_history, new_input], dim=-1)
        else:
            bot_input = new_input
        
        # ç”Ÿæˆå›å¤
        self.chat_history = self.model.generate(
            bot_input,
            max_length=1000,
            pad_token_id=self.tokenizer.eos_token_id,
            no_repeat_ngram_size=3,
            do_sample=True,
            top_k=50,
            top_p=0.95,
            temperature=0.7
        )
        
        # è§£ç å›å¤
        response = self.tokenizer.decode(
            self.chat_history[:, bot_input.shape[-1]:][0],
            skip_special_tokens=True
        )
        
        return response
    
    def reset(self):
        self.chat_history = None

# ä½¿ç”¨
bot = ChatBot()

print("ChatBot: ä½ å¥½ï¼æˆ‘æ˜¯æ™ºèƒ½åŠ©æ‰‹ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨çš„ï¼Ÿ")

while True:
    user_input = input("\nYou: ")
    
    if user_input.lower() in ['quit', 'exit', 'bye']:
        print("ChatBot: å†è§ï¼")
        break
    
    if user_input.lower() == 'reset':
        bot.reset()
        print("ChatBot: å¯¹è¯å·²é‡ç½®")
        continue
    
    response = bot.respond(user_input)
    print(f"ChatBot: {response}")
```

## ğŸ’¡ é¡¹ç›®æ€»ç»“

é€šè¿‡è¿™äº›å®æˆ˜æ¡ˆä¾‹ï¼Œä½ åº”è¯¥æŒæ¡äº†ï¼š

1. **å®Œæ•´NLPé¡¹ç›®æµç¨‹** - æ•°æ®å¤„ç†ã€æ¨¡å‹è®­ç»ƒã€è¯„ä¼°ã€éƒ¨ç½²
2. **æ–‡æœ¬åˆ†ç±»** - æ–°é—»åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æ
3. **ä¿¡æ¯æ£€ç´¢** - é—®ç­”ç³»ç»Ÿã€æ–‡æœ¬åŒ¹é…
4. **æ–‡æœ¬ç”Ÿæˆ** - æ‘˜è¦ç”Ÿæˆã€å¯¹è¯ç³»ç»Ÿ
5. **å·¥ç¨‹å®è·µ** - æ¨¡å‹ä¿å­˜ã€APIè®¾è®¡ã€æ€§èƒ½ä¼˜åŒ–

## ç»ƒä¹ é¡¹ç›®

1. **åƒåœ¾é‚®ä»¶æ£€æµ‹** - åˆ†ç±»åƒåœ¾é‚®ä»¶å’Œæ­£å¸¸é‚®ä»¶
2. **å…³é”®è¯æå–** - è‡ªåŠ¨æå–æ–‡ç« å…³é”®è¯
3. **æ–‡æœ¬çº é”™** - æ£€æµ‹å¹¶çº æ­£æ–‡æœ¬é”™è¯¯
4. **æ¨èç³»ç»Ÿ** - åŸºäºæ–‡æœ¬ç›¸ä¼¼åº¦çš„å†…å®¹æ¨è
5. **èˆ†æƒ…ç›‘æ§** - å®æ—¶åˆ†æç¤¾äº¤åª’ä½“æƒ…æ„Ÿ

---

**æ­å–œå®Œæˆ NLP æ•™ç¨‹ï¼** ğŸ‰

ç»§ç»­å­¦ä¹ ï¼š[è®¡ç®—æœºè§†è§‰ CV](../cv/) æˆ– [æ·±åº¦å­¦ä¹ æ¡†æ¶ PyTorch](../pytorch/)
