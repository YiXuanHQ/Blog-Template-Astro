---
title: RNNå’ŒLSTM
prevChapter: "python-ai/nlp/08-æ–‡æœ¬ç›¸ä¼¼åº¦"
nextChapter: "python-ai/nlp/10-Transformer"
parentChapter: "python-ai/nlp/README"
---
# RNN å’Œ LSTM

> æŒæ¡å¾ªç¯ç¥ç»ç½‘ç»œåœ¨NLPä¸­çš„åº”ç”¨

## ğŸ“š å­¦ä¹ ç›®æ ‡

- ç†è§£RNNåŸç†
- æŒæ¡LSTMæ¶æ„
- å­¦ä¼šåºåˆ—æ ‡æ³¨
- äº†è§£åŒå‘LSTM

## 1. RNN åŸºç¡€

```python
import torch
import torch.nn as nn

class SimpleRNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        self.hidden_size = hidden_size
        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        # x: (batch, seq_len, input_size)
        out, hidden = self.rnn(x)
        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        out = self.fc(out[:, -1, :])
        return out

# ä½¿ç”¨
model = SimpleRNN(input_size=100, hidden_size=128, output_size=2)
x = torch.randn(32, 10, 100)  # (batch, seq_len, features)
output = model(x)
print(output.shape)  # (32, 2)
```

## 2. LSTM

```python
class LSTMClassifier(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, num_classes)
    
    def forward(self, x):
        # x: (batch, seq_len)
        embedded = self.embedding(x)
        # LSTM
        lstm_out, (hidden, cell) = self.lstm(embedded)
        # ä½¿ç”¨æœ€åçš„éšè—çŠ¶æ€
        out = self.fc(hidden[-1])
        return out

# è®­ç»ƒ
model = LSTMClassifier(vocab_size=10000, embed_dim=100, 
                      hidden_dim=128, num_classes=2)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

for epoch in range(10):
    for batch_x, batch_y in train_loader:
        optimizer.zero_grad()
        outputs = model(batch_x)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
```

## 3. åŒå‘ LSTM

```python
class BiLSTM(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.bilstm = nn.LSTM(embed_dim, hidden_dim, 
                             batch_first=True, bidirectional=True)
        self.fc = nn.Linear(hidden_dim * 2, num_classes)  # *2 for bidirectional
    
    def forward(self, x):
        embedded = self.embedding(x)
        lstm_out, (hidden, cell) = self.bilstm(embedded)
        # æ‹¼æ¥å‰å‘å’Œåå‘çš„æœ€åéšè—çŠ¶æ€
        hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)
        out = self.fc(hidden)
        return out
```

## 4. åºåˆ—æ ‡æ³¨ï¼ˆNERï¼‰

```python
class BiLSTM_CRF(nn.Module):
    def __init__(self, vocab_size, tagset_size, embed_dim, hidden_dim):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.lstm = nn.LSTM(embed_dim, hidden_dim // 2,
                           num_layers=1, bidirectional=True, batch_first=True)
        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)
    
    def forward(self, x):
        embedded = self.embedding(x)
        lstm_out, _ = self.lstm(embedded)
        tag_scores = self.hidden2tag(lstm_out)
        return tag_scores

# ç”¨äºå‘½åå®ä½“è¯†åˆ«
model = BiLSTM_CRF(vocab_size=5000, tagset_size=10,
                   embed_dim=100, hidden_dim=256)
```

## 5. å®æˆ˜ï¼šæ–‡æœ¬åˆ†ç±»

```python
import torch
from torch.utils.data import Dataset, DataLoader
import jieba

# æ•°æ®é›†
class TextDataset(Dataset):
    def __init__(self, texts, labels, word2idx, max_len=50):
        self.texts = texts
        self.labels = labels
        self.word2idx = word2idx
        self.max_len = max_len
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        
        # åˆ†è¯å¹¶è½¬ä¸ºç´¢å¼•
        words = list(jieba.cut(text))
        indices = [self.word2idx.get(w, 0) for w in words]
        
        # æˆªæ–­æˆ–å¡«å……
        if len(indices) < self.max_len:
            indices += [0] * (self.max_len - len(indices))
        else:
            indices = indices[:self.max_len]
        
        return torch.tensor(indices), torch.tensor(label)

# è®­ç»ƒ
texts = ["è¿™ä¸ªäº§å“å¾ˆå¥½", "è´¨é‡å¤ªå·®äº†"]
labels = [1, 0]
word2idx = {"è¿™ä¸ª": 1, "äº§å“": 2, "å¾ˆ": 3, "å¥½": 4, "è´¨é‡": 5, "å¤ª": 6, "å·®": 7, "äº†": 8}

dataset = TextDataset(texts, labels, word2idx)
loader = DataLoader(dataset, batch_size=2, shuffle=True)

model = LSTMClassifier(vocab_size=len(word2idx)+1, embed_dim=50,
                      hidden_dim=64, num_classes=2)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters())

for epoch in range(10):
    for batch_x, batch_y in loader:
        optimizer.zero_grad()
        outputs = model(batch_x)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
    print(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")
```

---

**ä¸‹ä¸€èŠ‚ï¼š** [Transformer](10-Transformer.md)
